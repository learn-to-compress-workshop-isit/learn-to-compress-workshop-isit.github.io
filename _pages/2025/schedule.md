---
layout: page
permalink: /2025/schedule/
title: Schedule
description:
nav: true
nav_order: 3
---

The workshop will be held on **Thursday 26 June 2025** at the Rackham Graduate School, 915 E Washington St, Ann Arbor, MI 48109, USA. [See the venue on Google Maps](https://maps.app.goo.gl/QdavWxo9Qy9zWvvq6)

Some accepted papers are selected for spotlight presentations (see below).

For details on the invited talks and the speakers, please see the bottom of the page.

## Slides

The slides for some of the talks that were delivered at the workshop can be found here. For the spotlight presentations, names indicate the presenter.
- [**Invited Talk 1:** Prof. Shirin Saeedi Bidokhti: *Toward rate-distortion-perception optimality with lattice transform coding*](/assets/pdf/talk_slides/Shirin_Bidokhti-LearnToCompress.pdf)
- [**Invited Talk 2:** Prof. Ferenc Huszár: *New directions in causal inference via de Finetti theorems and algorithmic information theory*](/assets/pdf/talk_slides/Ferenc_Huszar-De_Finetti_Causal.pdf)
- [**Invited Talk 4:** Prof. Chao Tian: *Transformers learn to compress variable-order Markov chain sources in context*](/assets/pdf/talk_slides/Chao_Tian-CTW_TF.pdf)
- [**Invited Talk 6:** Dr. Yibo Yang: *Recent Advances in Diffusion-Based Generative Compression*](/assets/pdf/talk_slides/Yibo_Yang-ISIT_2025_talk.pdf)
- [**Spotlight Presentation 1:** Ali Zafari: *DeCompress: Denoising via Neural Compression*](/assets/pdf/talk_slides/Ali_Zafari-decompress_isit_workshop.pdf)
- [**Spotlight Presentation 3:** Matteo Zecchin: *Online Conformal Compression for Zero-Delay Communication with Distortion Guarantees*](/assets/pdf/talk_slides/Matteo_Zecchin-Online_Conformal_Compression.pdf)

## Timetable
<style>
table th:first-of-type {
    width: 20%;
}
table th:nth-of-type(2) {
    width: 30%;
}
table th:nth-of-type(3) {
    width: 50%;
}
a.speaker{
  scroll-margin-top: 150px;
}
</style>

| **Time (UTC-4)** | **Event** | **Invited Speaker / Spotlight Paper / Additional Notes** |
| :----------:   | :------- | :------- |
| 08:30 - 09:35 | [Shannon Lecture at the main conference venue](https://2025.ieee-isit.org/plenary-talks#shor) | |
| 09:35 - 09:55 | Moving / Coffee  break | |
| 09:55 - 10:00 | Opening remarks | | 
| 10:00 - 10:40 | **Invited Talk 1**  | [Prof. Shirin Saeedi Bidokhti: *Toward rate-distortion-perception optimality with lattice transform coding*](#shirin) |
| 10:40 - 11:20 | **Invited Talk 2** | [Prof. Ferenc Huszár: *New directions in causal inference via de Finetti theorems and algorithmic information theory*](#ferenc) |
| 11:20 - 11:50 | Poster Session 1 | Coffee break at the main venue until **11:30** |
| 11:50 - 12:30 | **Invited Talk 3** | [Dr. Pulkit Tandon: *Picking the right few: a statistical theory of data selection under weak supervision*](#pulkit) |
| 12:30 - 12:45 | **Spotlight presentation 1**  | Ali Zafari, Xi Chen and Shirin Jalali: *DeCompress: Denoising via Neural Compression* |
| 12:45 - 14:05 | Lunch Break | Overlaps with main conference lunch break between **12:50 - 2:30** |
| 14:05 - 14:45 | **Invited Talk 4**  | [Prof. Chao Tian: *Transformers learn to compress variable-order Markov chain sources in context*](#chao) |
| 14:45 - 15:00 | **Spotlight presentation 2** | Alfredo De la Fuente, Saurabh Singh and Jona Ballé: *Discretized Approximate Ancestral Sampling* |
| 15:00 - 15:15 | **Spotlight presentation 3** | Unnikrishnan Kunnath Ganesan, Giuseppe Durisi, Matteo Zecchin, Petar Popovski and Osvaldo Simeone: *Online Conformal Compression for Zero-Delay Communication with Distortion Guarantees* |
| 15:15 - 15:45 | Poster session 2  | |
| 15:45 - 16:25 | **Invited Talk 5**  | [Dr. Kedar Tatwawadi: *State of Learned Compression: Past, Present & Future*](#kedar) |
| 16:25 - 17:05 | **Invited Talk 6**  | [Dr. Yibo Yang: *Recent Advances in Diffusion-Based Generative Compression*](#yibo) |
| 17:05 - 17:10 | Awards and Closing Remarks | | 
| 17:10 - 17:30 | Poster Session 3 | | 
{: .table}
{: .table-striped}

The conference **banquet** is taking place directly after the workshop.


### **Accepted posters**:
<a name="acceptedPosters"></a>

- *Deep Randomized Distributed Function Computation (DeepRDFC): Neural Distributed Channel Simulation* \
Didrik Bergström and Onur Günlü [Poster #1]

- *Online Conformal Compression for Zero-Delay Communication with Distortion Guarantees* \
Unnikrishnan Kunnath Ganesan, Giuseppe Durisi, Matteo Zecchin, Petar Popovski and Osvaldo Simeone [Poster #2] [**spotlight**]

- *Gone with the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images* \
Tian Qiu, Arjun Nichani, Rasta Tadayontahmasebi and Haewon Jeong [Poster #3]

- *On List Decoding with Importance Sampling* \
 Buu Phan and Ashish Khisti [Poster #4]

- *LZMidi: Compression-Based Symbolic Music Generation* \
Connor Ding, Abhiram Rao Gorle, Sagnik Bhattacharya, Divija Hasteer, Naomi Sagan and Tsachy Weissman [Poster #5]

- *DeCompress: Denoising via Neural Compression* \
 Ali Zafari, Xi Chen and Shirin Jalali [Poster #6] [**spotlight**]

- *Generalizable Real-Time Accelerated Dynamic MRI* \
 Silpa Babu, Sajan Lingala and Namrata Vaswani [Poster #7]

- *Discretized Approximate Ancestral Sampling* \
 Alfredo De la Fuente, Saurabh Singh and Jona Ballé [Poster #8] [**spotlight**]

### **Keynotes**:

#### Invited Talk 1: *Toward rate-distortion-perception optimality with lattice transform coding*
<a name="shirin" class="speaker"></a>

**Speaker:** [Prof. Shirin Saeedi Bidokhti](https://www.seas.upenn.edu/~saeedi)

**Abstract**

Data-driven methods have been the driving force of many
scientific disciplines in the past decade, relying on huge amounts of
empirical, experimental, and scientific data. Working with big data is
impossible without data compression techniques that reduce the
dimension and size of the data for storage and communication purposes
and effectively denoise for efficient and accurate processing. In the
past decade, learning-based compressors such as nonlinear transform
coding (NTC) have shown great success in the task of compression by
learning to map a high dimensional source onto its representative
latent space of lower dimension using neural networks  and compressing
in that latent space.  However, despite their empirical success, NTC
and its variants are not optimal. To address this, we incorporate
fundamental principles from information theory into the design of
practical, learning-based compression methods. In particular, we
propose lattice transform coding, a principled framework that achieves
optimal rate-distortion and rate-distortion–perception tradeoffs for
Gaussian sources and demonstrates improvements over existing methods
on real-world datasets.

**Bio**

Shirin Saeedi Bidokhti is an associate professor in the Department of
Electrical and Systems Engineering at the University of Pennsylvania
(UPenn). She received her M.Sc. and Ph.D. degrees in Computer and
Communication Sciences from the Swiss Federal Institute of Technology
(EPFL). Prior to joining UPenn, she was a postdoctoral scholar at
Stanford University and the Technical University of Munich. She has
also held short-term visiting positions at ETH Zurich, University of
California at Los Angeles, and the Pennsylvania State University. Her
research interests broadly include the design and analysis of network
strategies that are scalable, practical, and efficient for use in
Internet of Things (IoT) applications, information transfer on
networks, as well as data compression techniques for big data. She is
a recipient of the 2023 Communications Society & Information Theory
Society Joint Paper Award, 2022 IT society Goldsmith lecturer award,
2021 NSF-CAREER award, 2019 NSF-CRII Research Initiative award and the
prospective researcher and advanced postdoctoral fellowships from the
Swiss National Science Foundation.

#### Invited Talk 2: *New directions in causal inference via de Finetti theorems and algorithmic information theory*
<a name="ferenc" class="speaker"></a>

**Speaker:** [Prof. Ferenc Huszár](https://www.inference.vc/about/)

**Abstract**

This talk will challenge well known results about the non-identifiably of causal relationships from observational data. Firstly, I will present how, by relaxing the common assumption that data is obtained i.i.d., one can perform causal identification using the so called causal de Finetti theorems. Then, pointing out the limitations of this approach, I will discuss how algorithmic information theory, and new notions such as algorithmic exchangeability may unlock new possibilities for analysing causal structures.

**Bio**

Ferenc Huszár is a Professor of Machine Learning at Cambridge University. His research focuses on understanding the limitations and behaviour of fearning methods, using empirical, theoretical and normative tools. Ferenc obtained a PhD in 2012 from Cambridge University and prior to taking up his current role, worked in the technology industry including deep learning startup Magic Pony Technology and Twitter.

#### Invited Talk 3: *Picking the right few: a statistical theory of data selection under weak supervision*
<a name="pulkit" class="speaker"></a>

**Speaker:** [Dr. Pulkit Tandon](https://www.linkedin.com/in/pulkit-tandon-8621a3a8/)

**Abstract**

Labeling and training costs now rival raw-data storage in modern learning pipelines. I will present a principled view of data selection as a complementary form of compression — one that trims the learning budget (labels, interactions, compute) rather than the bitrate. Given a vast unlabeled pool and a weak surrogate model that predicts better than chance, we ask: which n out of N points should we annotate to minimize test error? Through asymptotic analysis and experiments on toy and real-world image datasets, we derive closed-form performance curves and pinpoint regimes where biased selection outperforms popular unbiased heuristics such as influence-function-based sampling. We also uncover the counterintuitive finding that discarding large portions of data, or steering selection with a very weak surrogate, can actually boost generalization even in simple settings. I will close the talk by sketching how the same framework extends to recommender systems and other resource-constrained learning problems.

**Bio**

Pulkit Tandon is a Research Engineer at Granica. He leads the pilot rollout of the company’s data-selection product and conducts research on fundamental data optimization, scaling these methods to petabyte-scale AI workloads. He also served as an Adjunct Lecturer at Stanford University, teaching the graduate course “Data Compression: Theory and Applications.” Pulkit earned his Ph.D. in Electrical Engineering from Stanford and earlier contributed to Netflix’s video-encoding pipeline. His work bridges statistical learning and optimization with large-scale, real-world systems across both academia and industry.

#### Invited Talk 4: *Transformers learn to compress variable-order Markov chain sources in context*
<a name="chao" class="speaker"></a>

**Speaker:** [Prof. Chao Tian](https://tiangroup.engr.tamu.edu/)
 
**Abstract**

We study in-context learning of variable-length Markov chains by viewing language modeling as a form of data compression and focusing on the well-known variable-order Markov chain (VOMC) sources. This perspective allows classical compression algorithms, particularly the Bayesian optimal context-tree weighting (CTW) algorithm, to serve as baselines. We observe that single-layer transformers cannot learn VOMC in context, while transformers with two or more layers can indeed do so, with more layers providing small but noticeable improvements. Moreover, attention-only networks appear insufficient for this task. We analyze the attention map to extract meaningful structures and provide transformer constructions to explain the observed phenomena. A transformer construction with $D+2$ layers is given that can perform CTW accurately for VOMCs of maximum order $D$, and a simplified transformer construction utilizing partial information for approximate blending is studied to explain why 2-layer transformers work well.

**Bio**
Dr. Chao Tian received the B.E. degree in electronic engineering from Tsinghua University, and the M.S. and Ph.D. degrees in electrical and computer engineering from Cornell University. He was a Post-Doctoral Researcher at the Ecole Polytechnique Federale de Lausanne (EPFL), a member of Technical Staff—Research at AT&T Labs Research, and an Associate Professor with the Department of Electrical Engineering and Computer Science, The University of Tennessee Knoxville. He joined the Department of Electrical and Computer Engineering, Texas A&M University in 2017. He was an Associate Editor of the IEEE Signal Processing Letters from 2012 to 2014, an Editor of the IEEE Transactions on Communications from 2016 to 2021, and an Associate Editor for the IEEE Transactions on Information Theory from 2020-2023. He is currently an Associate Editor for the IEEE BITS Magazine and the ITSoc secretary.

#### Invited Talk 5: *State of Learned Compression: Past, Present & Future*
<a name="kedar" class="speaker"></a>

**Speaker:** [Dr. Kedar Tatwawadi](https://scholar.google.com/citations?user=RU0ZAp4AAAAJ&hl=en)

**Abstract**

In the past decade, ML based methods have introduced a powerful new paradigm for both lossless and lossy data compression. These learned approaches have enabled flexible, data-driven alternatives to traditional hand-crafted codecs, often matching or exceeding their performance in specific domains. In this talk, I will highlight a selection of landmark works in learned compression that have proven influential and enduring, setting foundational ideas that continue to shape the field. I will also present a few notable success stories where learned compression techniques have been deployed in real-world systems, shedding light on what made these deployments possible. Finally, I will discuss key challenges ahead as we reflect on what’s needed for learned compression to become a widely adopted part of future data infrastructure.

**Bio**

Kedar Tatwawadi is a ML Research Scientist at Apple. He leads a team of researchers who work on various problems related to ML-based image/video compression, enhancement and generation. Previously he completed his PhD under the guidance of Dr. Tsachy Weissman, and was a ML Researcher at WaveOne Inc, which specialized in ML-based video compression. His research interests lie at the intersection of machine learning and information theory, with a particular emphasis on data compression and statistical inference. His work bridges theory and practice, contributing to both foundational research and real-world systems.

#### Invited Talk 6: *Recent Advances in Diffusion-Based Generative Compression* 
<a name="yibo" class="speaker"></a>

**Speaker:** [Dr. Yibo Yang](https://yiboyang.com/about/)

**Abstract**

Popularized by their strong image generation performance, diffusion and related methods for mass transport ("diffusion" for short) have found widespread success in visual media applications.  In particular, diffusion methods have enabled new approaches to generative data compression, where realistic reconstructions can be achieved at extremely low bitrates.  In this talk, we will review recent diffusion-based methods for lossy compression, focusing on image compression with high realism. These methods generally employ diffusion in the decoding procedure to iteratively refine a compact data representation, such that the final reconstruction approximately follows the ground truth data distribution. The data representation can take on various forms and is typically transmitted via an auxiliary entropy model, with recent work exploring the use of diffusion models themselves for information transmission. We will discuss connections to inverse problem solving, rate-distortion-realism theory, architecture choices, and open research questions.

**Bio**

Yibo Yang is a research scientist on the AI/ML team at the Chan Zuckerberg Initiative, where he develops generative models and other machine-learning methods to accelerate biological discovery. He received his Ph.D. in Computer Science from the University of California, Irvine in 2024, advised by Stephan Mandt. His doctoral work established foundational connections between neural (learned) compression, deep generative modeling, and information theory, and he has led numerous tutorials and workshops in this area.