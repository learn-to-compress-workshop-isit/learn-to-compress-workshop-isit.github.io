---
layout: page
permalink: /2025/schedule/
title: Schedule
description:
nav: true
nav_order: 3
---

The workshop will be held on **Thursday 26 June 2025** at the Rackham Graduate School, 915 E Washington St, Ann Arbor, MI 48109, USA. [See the venue on Google Maps](https://maps.app.goo.gl/QdavWxo9Qy9zWvvq6)

Some accepted papers are selected for spotlight presentations (see below).

For details on the invited talks and the speakers, please see the bottom of the page.

<style>
table th:first-of-type {
    width: 20%;
}
table th:nth-of-type(2) {
    width: 30%;
}
table th:nth-of-type(3) {
    width: 50%;
}
a.speaker{
  scroll-margin-top: 150px;
}
</style>

| **Time (UTC-4)** | **Event** | **Invited Speaker / Spotlight Paper / Additional Notes** |
| :----------:   | :------- | :------- |
| 08:30 - 09:35 | [Shannon Lecture at the main conference venue](https://2025.ieee-isit.org/plenary-talks#shor) | |
| 09:35 - 09:55 | Moving / Coffee  break | |
| 09:55 - 10:00 | Opening remarks | | 
| 10:00 - 10:40 | **Invited Talk 1**  | [Prof. Shirin Saeedi Bidokhti](#shirin) |
| 10:40 - 11:20 | **Invited Talk 2** | [Prof. Ferenc Huszár: *New directions in causal inference via de Finetti theorems and algorithmic information theory*](#ferenc) |
| 11:20 - 11:50 | Poster Session 1 | Coffee break at the main venue until **11:30** |
| 11:50 - 12:30 | **Invited Talk 3** | [Dr. Pulkit Tandon: *Picking the right few: a statistical theory of data selection under weak supervision*](#pulkit) |
| 12:30 - 12:45 | **Spotlight presentation 1**  | Ali Zafari, Xi Chen and Shirin Jalali: *DeCompress: Denoising via Neural Compression* |
| 12:45 - 13:50 | Lunch Break | Overlaps with main conference lunch break between **12:50 - 2:30** |
| 13:50 - 14:30 | **Invited Talk 4**  | [Prof. Chao Tian: *Transformers learn to compress variable-order Markov chain sources in context*](#chao) |
| 14:30 - 14:45 | **Spotlight presentation 2** | Alfredo De la Fuente, Saurabh Singh and Jona Ballé: *Discretized Approximate Ancestral Sampling* |
| 14:45 - 15:00 | **Spotlight presentation 3** | Unnikrishnan Kunnath Ganesan, Giuseppe Durisi, Matteo Zecchin, Petar Popovski and Osvaldo Simeone: *Online Conformal Compression for Zero-Delay Communication with Distortion Guarantees* |
| 15:00 - 15:30 | Poster session 2  | |
| 15:30 - 16:10 | **Invited Talk 5**  | [Dr. Kedar Tatwawadi: *State of Learned Compression: Past, Present & Future*](#kedar) |
| 16:10 - 16:50 | **Invited Talk 6**  | [Dr. Yibo Yang](#yibo) |
| 16:50 - 16:55 | Awards and Closing Remarks | | 
| 16:55 - 17:25 | Poster Session 3 | | 
{: .table}
{: .table-striped}

The conference **banquet** is taking place directly after the workshop.


### **Accepted posters**:
<a name="acceptedPosters"></a>

- *Deep Randomized Distributed Function Computation (DeepRDFC): Neural Distributed Channel Simulation* \
Didrik Bergström and Onur Günlü [Poster #1]

- *Online Conformal Compression for Zero-Delay Communication with Distortion Guarantees* \
Unnikrishnan Kunnath Ganesan, Giuseppe Durisi, Matteo Zecchin, Petar Popovski and Osvaldo Simeone [Poster #2] [**spotlight**]

- *Gone with the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images* \
Tian Qiu, Arjun Nichani, Rasta Tadayontahmasebi and Haewon Jeong [Poster #3]

- *On List Decoding with Importance Sampling* \
 Buu Phan and Ashish Khisti [Poster #4]

- *LZMidi: Compression-Based Symbolic Music Generation* \
Connor Ding, Abhiram Rao Gorle, Sagnik Bhattacharya, Divija Hasteer, Naomi Sagan and Tsachy Weissman [Poster #5]

- *DeCompress: Denoising via Neural Compression* \
 Ali Zafari, Xi Chen and Shirin Jalali [Poster #6] [**spotlight**]

- *Generalizable Real-Time Accelerated Dynamic MRI* \
 Silpa Babu, Sajan Lingala and Namrata Vaswani [Poster #7]

- *Discretized Approximate Ancestral Sampling* \
 Alfredo De la Fuente, Saurabh Singh and Jona Ballé [Poster #8] [**spotlight**]

### **Keynotes**:

#### Invited Talk 1: TBA
<a name="shirin" class="speaker"></a>

**Speaker:** [Prof. Shirin Saeedi Bidokhti](https://www.seas.upenn.edu/~saeedi)

#### Invited Talk 2: New directions in causal inference via de Finetti theorems and algorithmic information theory
<a name="ferenc" class="speaker"></a>

**Speaker:** [Prof. Ferenc Huszár](https://www.inference.vc/about/)

**Abstract**

This talk will challenge well known results about the non-identifiably of causal relationships from observational data. Firstly, I will present how, by relaxing the common assumption that data is obtained i.i.d., one can perform causal identification using the so called causal de Finetti theorems. Then, pointing out the limitations of this approach, I will discuss how algorithmic information theory, and new notions such as algorithmic exchangeability may unlock new possibilities for analysing causal structures.

**Bio**

Ferenc Huszár is a Professor of Machine Learning at Cambridge University. His research focuses on understanding the limitations and behaviour of fearning methods, using empirical, theoretical and normative tools. Ferenc obtained a PhD in 2012 from Cambridge University and prior to taking up his current role, worked in the technology industry including deep learning startup Magic Pony Technology and Twitter.

#### Invited Talk 3: *Picking the right few: a statistical theory of data selection under weak supervision*
<a name="pulkit" class="speaker"></a>

**Speaker:** [Dr. Pulkit Tandon](https://www.linkedin.com/in/pulkit-tandon-8621a3a8/)

**Abstract**

Labeling and training costs now rival raw-data storage in modern learning pipelines. I will present a principled view of data selection as a complementary form of compression — one that trims the learning budget (labels, interactions, compute) rather than the bitrate. Given a vast unlabeled pool and a weak surrogate model that predicts better than chance, we ask: which n out of N points should we annotate to minimize test error? Through asymptotic analysis and experiments on toy and real-world image datasets, we derive closed-form performance curves and pinpoint regimes where biased selection outperforms popular unbiased heuristics such as influence-function-based sampling. We also uncover the counterintuitive finding that discarding large portions of data, or steering selection with a very weak surrogate, can actually boost generalization even in simple settings. I will close the talk by sketching how the same framework extends to recommender systems and other resource-constrained learning problems.

**Bio**

Pulkit Tandon is a Research Engineer at Granica. He leads the pilot rollout of the company’s data-selection product and conducts research on fundamental data optimization, scaling these methods to petabyte-scale AI workloads. He also served as an Adjunct Lecturer at Stanford University, teaching the graduate course “Data Compression: Theory and Applications.” Pulkit earned his Ph.D. in Electrical Engineering from Stanford and earlier contributed to Netflix’s video-encoding pipeline. His work bridges statistical learning and optimization with large-scale, real-world systems across both academia and industry.

#### Invited Talk 4: *Transformers learn to compress variable-order Markov chain sources in context*
<a name="chao" class="speaker"></a>

**Speaker:** [Prof. Chao Tian](https://tiangroup.engr.tamu.edu/)
 
**Abstract**

We study in-context learning of variable-length Markov chains by viewing language modeling as a form of data compression and focusing on the well-known variable-order Markov chain (VOMC) sources. This perspective allows classical compression algorithms, particularly the Bayesian optimal context-tree weighting (CTW) algorithm, to serve as baselines. We observe that single-layer transformers cannot learn VOMC in context, while transformers with two or more layers can indeed do so, with more layers providing small but noticeable improvements. Moreover, attention-only networks appear insufficient for this task. We analyze the attention map to extract meaningful structures and provide transformer constructions to explain the observed phenomena. A transformer construction with $D+2$ layers is given that can perform CTW accurately for VOMCs of maximum order $D$, and a simplified transformer construction utilizing partial information for approximate blending is studied to explain why 2-layer transformers work well.

**Bio**
Dr. Chao Tian received the B.E. degree in electronic engineering from Tsinghua University, and the M.S. and Ph.D. degrees in electrical and computer engineering from Cornell University. He was a Post-Doctoral Researcher at the Ecole Polytechnique Federale de Lausanne (EPFL), a member of Technical Staff—Research at AT&T Labs Research, and an Associate Professor with the Department of Electrical Engineering and Computer Science, The University of Tennessee Knoxville. He joined the Department of Electrical and Computer Engineering, Texas A&M University in 2017. He was an Associate Editor of the IEEE Signal Processing Letters from 2012 to 2014, an Editor of the IEEE Transactions on Communications from 2016 to 2021, and an Associate Editor for the IEEE Transactions on Information Theory from 2020-2023. He is currently an Associate Editor for the IEEE BITS Magazine and the ITSoc secretary.

#### Invited Talk 5: *State of Learned Compression: Past, Present & Future*
<a name="kedar" class="speaker"></a>

**Speaker:** [Dr. Kedar Tatwawadi](https://scholar.google.com/citations?user=RU0ZAp4AAAAJ&hl=en)

**Abstract**

In the past decade, ML based methods have introduced a powerful new paradigm for both lossless and lossy data compression. These learned approaches have enabled flexible, data-driven alternatives to traditional hand-crafted codecs, often matching or exceeding their performance in specific domains. In this talk, I will highlight a selection of landmark works in learned compression that have proven influential and enduring, setting foundational ideas that continue to shape the field. I will also present a few notable success stories where learned compression techniques have been deployed in real-world systems, shedding light on what made these deployments possible. Finally, I will discuss key challenges ahead as we reflect on what’s needed for learned compression to become a widely adopted part of future data infrastructure.

**Bio**

Kedar Tatwawadi is a ML Research Scientist at Apple. He leads a team of researchers who work on various problems related to ML-based image/video compression, enhancement and generation. Previously he completed his PhD under the guidance of Dr. Tsachy Weissman, and was a ML Researcher at WaveOne Inc, which specialized in ML-based video compression. His research interests lie at the intersection of machine learning and information theory, with a particular emphasis on data compression and statistical inference. His work bridges theory and practice, contributing to both foundational research and real-world systems.

#### Invited Talk 6: TBA
<a name="yibo" class="speaker"></a>

**Speaker:** [Dr. Yibo Yang](https://yiboyang.com/about/)