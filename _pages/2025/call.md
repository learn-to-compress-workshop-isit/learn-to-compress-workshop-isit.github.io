---
layout: page
permalink: /2025/submissions/
title: Call for Papers
description:  
nav: true
nav_order: 3
---
The exponential growth of global data has intensified the demand for efficient data compression, with deep learning techniques like variational autoencoders, generative adversarial networks (GANs), and diffusion models reshaping traditional approaches to source coding. These learning-based methods have demonstrated the potential to outperform traditional codecs across various data modalities like images, video, and audio. However, challenges remain in improving computational efficiency, understanding the theoretical limits of neural compression and compression without quantization as well as addressing issues in distributed settings.

In parallel, compression has emerged as a powerful proxy task for advancing broader learning objectives, including as representation learning and model efficiency. Recent research is exploring how compression can enhance training and generalization of large-scale foundation models for vision, language, and multi-modal applications. Techniques like knowledge distillation, model pruning, and quantization share common challenges with compression, highlighting the symbiotic relationship between these seemingly distant concepts. The intersection of learning, compression and information theory offers exciting new avenues for advancing both practical compression techniques and also our understanding of deep learning dynamics.

This workshop aims to unite experts from machine learning, computer science, and information theory to delve into the dual themes of _learning-based compression_ and using _compression as a tool for learning tasks_.

#### Topics of interest include, but are not limited to:

 - __Theoretical Foundations of Neural Compression:__ Fundamental limits (e.g., rate-distortion bounds),  distortion/perceptual/realism metrics, distributed compression, compression without quantization (e.g., channel simulation, relative entropy coding), and stochastic/probabilistic coding techniques.
 - __Compression as a Tool for Learning:__ Leveraging principles of compression and source coding to understand and improve learning and generalization
 - __Contributions in Learning-Based Data Compression:__ New techniques for compressing data (e.g., images, video, audio), model weights, and emerging modalities (e.g., 3D content and AR/VR applications)
 - __Compression as a Proxy for Learning:__ Understanding the information-theoretic role of compression in tasks like unsupervised learning, representation learning, and semantic understanding.
 - __Efficiency for Large-Scale Models:__ Accelerating training and inference for large foundation models, particularly in distributed and resource-constrained settings
 - __Interplay of Algorithmic Information Theory and Source Coding:__ Exploring connections between Algorithmic Information Theory concepts (e.g., Kolmogorov complexity, Solomonoff induction) and emerging source coding methods

<!--
We invite researchers from data compression and related fields to submit their latest work on theory and applications of data compression to the workshop. All accepted papers will be presented as posters during the poster session. Some papers will also be selected for **spotlight presentations**.

Topics of interest include but are not limited to:
* Source coding (e.g., images, video, abstract sources) in the era of machine learning.
* Learning-based approaches to information theory.
* Theoretical foundations of learned compression.
* Quantization, entropy coding, stochastic coding, and channel synthesis.
* Compression under perceptual/realism metrics.
* Learning-based approaches to distributed and multi-terminal compression.


## Important Dates
* Paper submission deadline: ~~March 3, 2024 (11:59 PM, anywhere in the world!).~~ 
**March 17, 2024 (11:59 PM, anywhere in the world!)**
* ~~Decision notification: April 15, 2024.~~
* Camera-ready paper deadline: May 6, 2024.
* ISIT early registration deadline: May 6, 2024.
* Workshop date: July 7, 2024.

## Submission Details

The authors can submit either an extended abstract (up to 3 pages) or a full-paper (up to 5 pages of main text, one optional page containing only references, and an optional 5-page appendix for
further details, proofs etc.). All submissions should be prepared in the ISIT paper format. You can find information for authors such as paper format, template and example in this [link](https://2024.ieee-isit.org/information-authors-0). All submissions (papers and extended abstracts) should be made through our venue home page on [OpenReview](https://openreview.net/group?id=IEEE.org/ISIT/2024/Workshop/LCW#tab-your-consoles). 


Each paper will go through a rigorous review process. The workshop will follow a **single-blind reviewing policy**, aligned with the ISIT 2024, which means that the all submitted manuscripts should include author names and affiliations. The authors can post their papers on arXiv if they wish to do so. 

For full-paper submissions, we will offer authors the choice to publish their accepted papers on IEEE Xplore. All accepted submissions (extended abstract or full-paper) will also be published via OpenReview after the workshop. 

We welcome all relevant submissions that have been presented, published or are currently undergoing review elsewhere, *if the authors decide not to publish their full-paper on IEEE Xplore*.

An author of an accepted paper or extended abstract must register to the workshop and present a poster. For some selected papers, there will be a spotlight presentation (see [Schedule](https://learn-to-compress-workshop-isit.github.io/schedule/)). To maintain the interactive nature of the workshop, we kindly request all presentations to be **in-person**.  

Only accepted papers that are presented will be published on IEEE Xplore. The requirements of the poster will be communicated with the acceptance notification for the paper. 





## Questions

If you are interested in reviewing or if you have any questions, please do not hesitate to contact us at [learn.to.compress.workshop@gmail.com](mailto:learn.to.compress.workshop@gmail.com).
-->