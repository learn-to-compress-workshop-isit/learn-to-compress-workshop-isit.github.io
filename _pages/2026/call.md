---
layout: page
permalink: /2026/call/
title: Call for Papers
description:  
nav: true
nav_order: 2
---
The exponential growth of global data has intensified the demand for efficient data compression, with deep learning techniques like variational autoencoders, generative adversarial networks (GANs), diffusion models, and implicit neural representations reshaping traditional approaches to source coding. Learning-based neural compression methods have demonstrated the potential to outperform traditional codecs across various data modalities like image, video, and audio. However, challenges remain in improving their computational efficiency and memory requirements, understanding the theoretical limits of neural compression and compression without quantization as well as addressing challenges in distributed settings.

In parallel, compression has emerged as a powerful proxy task for advancing broader learning objectives, including representation learning and model efficiency. Recent research is exploring how compression can enhance training and generalization of large-scale foundation models for vision, language, and multi-modal applications. Techniques like knowledge distillation, model pruning, and quantization share common challenges with compression, highlighting the symbiotic relationship between these seemingly distant concepts. The intersection of learning, compression and information theory offers exciting new avenues for advancing both practical compression techniques and also our understanding of deep learning dynamics.

This workshop aims to unite experts from machine learning, computer science, and information theory to delve into the dual themes of _learning-based compression_ and using _compression as a tool for learning tasks_.


## Topics of interest include, but are not limited to:

- __``Learn to Compress” – Advancing Compression with Learning__
  - __Learning-Based Data Compression:__ New techniques for compressing data (e.g., images, video, audio), model weights, and emerging modalities (e.g., 3D content and AR/VR applications).
  - __Efficiency for Large-Scale Foundation Models:__ Accelerating training and inference for large-scale foundation models, particularly in distributed and resource-constrained settings
  - __Theoretical Foundations of Neural Compression:__ Fundamental limits (e.g., rate-distortion bounds),  distortion/perceptual/realism metrics, distributed compression, compression without quantization (e.g., channel simulation, relative entropy coding), and stochastic/probabilistic coding techniques.
- __``Compress to Learn” – Leveraging Principles of Compression to Improve Learning__
  - __Compression as a Tool for Learning:__ Leveraging principles of compression and source coding to understand and improve learning and generalization.
  - __Compression as a Proxy for Learning:__ Understanding the information-theoretic role of compression in tasks like unsupervised learning, representation learning, and semantic understanding.
  - __Interplay of Algorithmic Information Theory and Source Coding:__ Exploring connections between Algorithmic Information Theory concepts (e.g., Kolmogorov complexity, Solomonoff induction) and emerging source coding methods.

All accepted papers will be presented as posters during the poster session. We welcome all relevant recent submissions that have been presented, published or are currently undergoing review elsewhere, if the authors decide not to publish their full-paper on IEEE Xplore. Some papers will also be selected for spotlight presentations.

## Important Dates
- Paper submission deadline: <span style="color:red;font-weight:bold;"><s style="text-decoration-thickness: 2px;">March 14</s> March 28, 2025</span> (11:59 PM, anywhere in the world!).
- Decision notification: __April 18, 2025__
- Camera-ready paper deadline: __May 1, 2025__
- Workshop date: __June 26, 2025__

## Submission Details

All submitted papers should be prepared in the ISIT 2025 paper format. You can find information for authors such as paper format, template and example **[at this link](https://2025.ieee-isit.org/information-authors-0)**. 


<b>All papers should be made through our venue home page via <a href="https://edas.info/N33111" target="_blank">this EDAS link</a>.</b> 
<i>Note: The EDAS submission form is currently titled "Register a paper for 2025 IEEE International Symposium on Information Theory (ISIT)", without reference to the workshop. Please disregard this; you can safely register your paper(s) through the form, they will be correctly assigned to the workshop. </i>



Each paper will go through a rigorous review process. The workshop will follow a __single-blind reviewing policy__, aligned with the ISIT 2025, which means that the all submitted manuscripts should include author names and affiliations. The authors can post their papers on arXiv if they wish to do so.

We will offer authors the choice to publish their accepted papers on IEEE Xplore.

We welcome all relevant submissions that have been presented, published or are currently undergoing review elsewhere, if the authors decide not to publish their full-paper on IEEE Xplore.

An author of an accepted paper must register to the workshop and present a poster. For some selected papers, there will be a spotlight presentation. To maintain the interactive nature of the workshop, we kindly request all presentations to be __in-person__.

Only accepted papers that are presented will be published on IEEE Xplore. The requirements of the poster will be communicated with the acceptance notification for the paper.